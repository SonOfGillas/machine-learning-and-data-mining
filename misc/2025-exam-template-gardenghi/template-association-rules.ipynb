{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb03579b",
   "metadata": {},
   "source": [
    "# Association Rules Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25c87e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from mlxtend.frequent_patterns import apriori,association_rules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54507deb",
   "metadata": {},
   "source": [
    "## Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669bc80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read Data\n",
    "url = 'file.xlsx'\n",
    "df = pd.read_excel(url)  #pd.read_csv\n",
    "print(f\"There are {df.shape[0]} rows and {df.shape[1]} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae6377a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the first 5 row of the dataset\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2809a0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c085d46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#n rows with missing values\n",
    "df.shape[0]-df.dropna().shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff35ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of missing values per columns\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7660b610",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of unique values per column\n",
    "df.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cbd3ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the number of rows that contains a specific value\n",
    "\n",
    "value_to_check = 'value'\n",
    "value_to_check_int = 0\n",
    "column_to_scan = 'column_name'\n",
    "# for string\n",
    "sum(df[column_to_scan].str.contains(value_to_check))\n",
    "# for integer\n",
    "sum(df[column_to_scan] == value_to_check_int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b571273d",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29fe2ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove leading and trailing spaces from string columns\n",
    "column_to_strip = 'column_name' \n",
    "df[column_to_strip] = df[column_to_strip].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385817e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all the rows that miss a specific feature (a specific feature is null)\n",
    "column_that_should_not_be_null = 'example_column'\n",
    "df1 = df.dropna(axis=0, subset=[column_that_should_not_be_null])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5117b6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all the rows with a specific string\n",
    "# ~ is Alt+126\n",
    "string_to_check=''\n",
    "var_to_check=0\n",
    "column_to_scan = ''\n",
    "# for strings\n",
    "df1 = df[~df[column_to_scan].str.contains(string_to_check)]\n",
    "# for integer\n",
    "df1 = df[df[column_to_scan] != var_to_check]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc36661b",
   "metadata": {},
   "source": [
    "### BASKET CREATION first method: ustack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caedb495",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the explanation for this code is below the code\n",
    "\n",
    "# CREATE THE BASKET\n",
    "# You're essentially transforming a \"long\" list of individual transactions \n",
    "# into a \"wide\" matrix where each row is an invoice and each column is a product.\n",
    "\n",
    "# invoceNo - transaction-id (the same transaction could be split in more rows)\n",
    "# Description - elemenet of the transaction\n",
    "# quantity - quantity of the element in the transaction\n",
    "\n",
    "basket = (df\n",
    "          .groupby(['InvoiceNo', 'Description'])['Quantity']\n",
    "          .sum().unstack().reset_index().fillna(0)\n",
    "          .set_index('InvoiceNo') # in this way, InvoiceNo is not a column anymore\n",
    "          # .astype(bool) <- you can directly use it instead of the encoder in the next step\n",
    "          )\n",
    "\n",
    "\n",
    "basket.head()\n",
    "\n",
    "\n",
    "# IF THERE IS NO QUANTITY feature :\n",
    "# use .size() directly\n",
    "\n",
    "basket = (\n",
    "    df.groupby(['transaction_id', 'element']).size()\n",
    "    .unstack()\n",
    "    .reset_index().fillna(0)\n",
    "    .set_index('transaction_id')\n",
    "    # .astype(bool) <- you can directly use it instead of the encoder in the next step\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e3ad51",
   "metadata": {},
   "source": [
    "#### Basket Creation: Code Explained\n",
    "\n",
    "\n",
    "### 1. The Original Data (`df`)\n",
    "\n",
    "This is an exemple DF. Notice how **Invoice 102** appears twice because they bought Apples in two separate batches.\n",
    "\n",
    "| InvoiceNo | Description | Quantity |\n",
    "| --- | --- | --- |\n",
    "| 101 | Apple | 2 |\n",
    "| 101 | Bread | 1 |\n",
    "| **102** | **Apple** | **5** |\n",
    "| **102** | **Apple** | **3** |\n",
    "| 103 | Bread | 2 |\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Group & Sum\n",
    "\n",
    "`df.groupby(['InvoiceNo', 'Description'])['Quantity'].sum()`\n",
    "\n",
    "This step collapses the duplicates. It creates a **MultiIndex** (a nested hierarchy).\n",
    "\n",
    "| InvoiceNo | Description | **Quantity (Sum)** |\n",
    "| --- | --- | --- |\n",
    "| **101** | Apple | 2 |\n",
    "|  | Bread | 1 |\n",
    "| **102** | Apple | **8** (5+3) |\n",
    "| **103** | Bread | 2 |\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Unstack\n",
    "\n",
    "`.unstack()`\n",
    "\n",
    "This is the \"Pivot.\" The `Description` labels (Apple, Bread) jump from being **rows** to being **column headers**.\n",
    "\n",
    "| InvoiceNo | **Apple** | **Bread** |\n",
    "| --- | --- | --- |\n",
    "| **101** | 2.0 | 1.0 |\n",
    "| **102** | 8.0 | *NaN* |\n",
    "| **103** | *NaN* | 2.0 |\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Reset Index & Fillna\n",
    "\n",
    "`.reset_index().fillna(0)`\n",
    "\n",
    "First, we flatten the table (giving it a standard row count 0, 1, 2). Then, we replace the empty `NaN` holes with `0` so the math works later.\n",
    "\n",
    "|  | InvoiceNo | Apple | Bread |\n",
    "| --- | --- | --- | --- |\n",
    "| **0** | 101 | 2.0 | 1.0 |\n",
    "| **1** | 102 | 8.0 | **0.0** |\n",
    "| **2** | 103 | **0.0** | 2.0 |\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Final Set Index\n",
    "\n",
    "`.set_index('InvoiceNo')`\n",
    "\n",
    "This is the final \"Basket.\" The `InvoiceNo` is no longer a data column; it is the **label** for the row. This leaves only the product quantities in the main body of the table.\n",
    "\n",
    "| **InvoiceNo** (Index) | Apple | Bread |\n",
    "| --- | --- | --- |\n",
    "| **101** | 2.0 | 1.0 |\n",
    "| **102** | 8.0 | 0.0 |\n",
    "| **103** | 0.0 | 2.0 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d23424d",
   "metadata": {},
   "source": [
    "## Basket Creation Second Method: get_dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48e9e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# THE PROFESSOR NEVER EXPLAINED THIS METHOD IN 2026, BUT I THINCK IT EXPLAINED IT IN PAST YEARS, SO MAYBE CAN BE USEFULL.\n",
    "df0 = pd.get_dummies(df, prefix='', prefix_sep='', dummy_na=False)\n",
    "df1 = df0.drop(columns=['element']) # We drop the items column, as we dont need that information. \n",
    "\n",
    "# IMPORTANT!!!!\n",
    "df1 = df1.groupby(level=0, axis=1).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55477428",
   "metadata": {},
   "source": [
    "the `get_dummies` approach\n",
    "\n",
    "Imagine we have this small dataframe (`df`):\n",
    "\n",
    "| transaction_id | element |\n",
    "| --- | --- |\n",
    "| 1 | Milk |\n",
    "| 1 | Bread |\n",
    "| 2 | Milk |\n",
    "\n",
    "---\n",
    "\n",
    "## Step-by-Step Transformation\n",
    "\n",
    "### 1. `pd.get_dummies(df, ...)`\n",
    "\n",
    "This step looks at the `element` column and creates a new column for every unique item it finds.\n",
    "\n",
    "| transaction_id | Bread | Milk |\n",
    "| --- | --- | --- |\n",
    "| 1 | 0 | 1 |\n",
    "| 1 | 1 | 0 |\n",
    "| 2 | 0 | 1 |\n",
    "\n",
    "> **Note:** Notice how Transaction 1 now has two separate rows. This is because the original data had two rows for that ID.\n",
    "\n",
    "### 2. `df1.groupby(level=0, axis=1).sum()`\n",
    "\n",
    "The \"Magic\" happens here. Since we want one row per transaction, we need to collapse (group) the rows. If we group by `transaction_id` and sum the columns, we get our final basket:\n",
    "\n",
    "| transaction_id | Bread | Milk |\n",
    "| --- | --- | --- |\n",
    "| **1** | 1 | 1 |\n",
    "| **2** | 0 | 1 |\n",
    "\n",
    "---\n",
    "\n",
    "### Why use this over Method 1?\n",
    "\n",
    "In **Method 1**, you used `.unstack()`. Unstacking is like rotating a specific column.\n",
    "**Method 2** (`get_dummies`) is like exploding the column.\n",
    "\n",
    "If your data is already \"clean\" (one item per row), Method 2 is often shorter to write. However, it can be much slower if you have a huge number of unique items (e.g., a supermarket with 50,000 different products), because it creates a massive matrix in your computer's memory before it finishes the grouping step.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acdb79e5",
   "metadata": {},
   "source": [
    "## Basket creation third method: oneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23737ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## IN SOME OLD EXAM THE PROFESSOR ASK FOR THIS METHOD. BUT WE NEVER DID IT IN 2026\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Setup the Encoder\n",
    "# sparse_output=False gives us a readable array; handle_unknown ignores new items in future data\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "\n",
    "# 2. Fit and Transform the 'element' column\n",
    "# We reshape to (-1, 1) because the encoder expects a 2D input\n",
    "encoded_array = encoder.fit_transform(df[['element']])\n",
    "\n",
    "# 3. Create a DataFrame from the result\n",
    "# We use encoder.get_feature_names_out() to get the item names back\n",
    "encoded_df = pd.DataFrame(\n",
    "    encoded_array, \n",
    "    columns=encoder.get_feature_names_out(['element']),\n",
    "    index=df['transaction_id']\n",
    ")\n",
    "\n",
    "# 4. Group by transaction_id to create the basket\n",
    "basket = encoded_df.groupby('transaction_id').sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f99ec1b",
   "metadata": {},
   "source": [
    "## Converion Basket To Binary values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00a1c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove quantities from basket, we need only the prence of the element in the transaction\n",
    "def encode_units(x):\n",
    "    if x >= 1:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "basket_sets = basket.map(encode_units)\n",
    "\n",
    "# basket_sets.describe()\n",
    "# besket_sets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51de9070",
   "metadata": {},
   "source": [
    "## Rule Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8736fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 0.01\n",
    "min_support = 1\n",
    "min_rules = 20\n",
    "metric = 'lift'\n",
    "min_threshold = 1\n",
    "\n",
    "while True:\n",
    "    frequent_itemsets = apriori(basket_sets,min_support=min_support,use_colnames=True)\n",
    "    if frequent_itemsets.shape[0] > 0:\n",
    "        rules = association_rules(frequent_itemsets,metric=metric,min_threshold=min_threshold)\n",
    "    if frequent_itemsets.shape[0] > 0 and rules.shape[0] >= min_rules:\n",
    "        break\n",
    "    else:\n",
    "        min_support -= step\n",
    "\n",
    "print(\"'min_support'={:4.2f}, number of frequent itemsets={}, number of rules={}\"\\\n",
    "      .format(min_support, frequent_itemsets.shape[0], rules.shape[0]))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867d3949",
   "metadata": {},
   "outputs": [],
   "source": [
    "rules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a644a210",
   "metadata": {},
   "source": [
    "### Display Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8e0e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the rules by lift and confidence\n",
    "sorted_rules=rules.sort_values(by=['lift','confidence'],ascending=False).reset_index(drop=True)\n",
    "sorted_rules.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ebb6159",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a lift vs confidence scatter plot, where the size of the points is proportional to the support of the rule\n",
    "sorted_rules.plot.scatter(x='lift',y='confidence',s=3**(sorted_rules['support']*30),title='Association Rules')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad2fdfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a lift vs confidence scatter plot, where the size of the points is proportional to the support of the rule\n",
    "s = [1.8**n for n in rules.lift]\n",
    "rules.plot.scatter(x='support', \n",
    "                   y='confidence', \n",
    "                   title='Association Rules (dot proportional to Lift)', \n",
    "                   s=s)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLDM2526",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
