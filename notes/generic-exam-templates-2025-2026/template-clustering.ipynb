{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4cc66da7",
   "metadata": {},
   "source": [
    "# Clustering Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08335a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "randomState=42\n",
    "np.random.seed(randomState)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165a867c",
   "metadata": {},
   "source": [
    "## Esplore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b5e068",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = ''\n",
    "df = pd.read_csv(url) #delimiter=, index_col=, names=\n",
    "\n",
    "print(f' Data frame has {df.shape[0]} samples, and {df.shape[1]-1} features ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aede31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the first 5 rows of the dataset\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebeecf88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# explore the distribution of the target variable\n",
    "# count help to see if there are some missing values\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8327ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# n rows with missing values\n",
    "df.shape[0]-df.dropna().shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c62076f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of missing values per columns\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af2e707",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the distribution of the features\n",
    "# check for outliers and different scales of the features\n",
    "df.boxplot(figsize=(15,10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8486037b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize linear relationship between features\n",
    "sns.pairplot(df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ecba32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# is some feature have different value for every row/sample we eliminate it\n",
    "df['Territorio'].unique().shape\n",
    "\n",
    "#### or ####\n",
    "\n",
    "df.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0808c73",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223975df",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'there are {df.isna().sum().sum()} null values')\n",
    "df1 = df.copy().dropna()\n",
    "print(f'there are {df1.isna().sum().sum()} null values')\n",
    "print(f'Data frame has {df1.shape[0]} samples, and {df1.shape[1]} features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66dd276",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (OPTIONAL) If there is a string variable, we need to encode it to numerical values \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "column_to_transform = ''\n",
    "transformed_column = le.fit_transform(df1[column_to_transform].values)\n",
    "df1[column_to_transform] = transformed_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0770497",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (OPTIONAL) use this to convert nominal labels to numerical values\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "one = OneHotEncoder()\n",
    "column_to_transform = 'exemple_column'\n",
    "enc_data = one.fit_transform(df[column_to_transform].values)\n",
    "l = list(one.categories_[0])\n",
    "enc_df = pd.DataFrame(enc_data.toarray(),columns=l)\n",
    "df = df.join(enc_df)\n",
    "df = df.drop([column_to_transform],axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30e6d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (OPTIONAL) use this to convert ordinal labels to numerical values\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "categories = ['bad','good','very good'] # exemple of ordinal categories\n",
    "oe = OrdinalEncoder(categories=categories,dtype=int)\n",
    "column_to_transform = 'col_name'\n",
    "df[column_to_transform] = oe.fit_transform(df[column_to_transform].values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da01c878",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the ranges of the features to be between 0 and 1\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "df_processed = pd.DataFrame(scaler.fit_transform(df),columns=df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9feffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data standardization\n",
    "from sklearn.preprocessing import PowerTransformer,StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "preprocessor = make_pipeline(PowerTransformer(),StandardScaler())\n",
    "df_processed = pd.DataFrame(preprocessor.fit_transform(df),columns=df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4fba78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (OPTIONAL) DO THIS STEP ONLY IF THE DATASET HAS A LARGE NUMBER OF FEATURES (E.G. MORE THAN 20)\n",
    "# remove features with low variance, or with high correlation with other features\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA()\n",
    "df_tranformed = pca.fit_transform(df)\n",
    "print(f'Explained variance ration: {pca.explained_variance_ration_}')\n",
    "min_variance = 0.9 # or 0.8\n",
    "variance_cumsum = np.cumsum(pca.explained_variance_ration_.copy())\n",
    "cutoff_index = np.argmax(variance_cumsum>min_variance)\n",
    "df = df_tranformed[:,:cutoff_index+1]\n",
    "print(f'df shape after PCA {df.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7783dbb",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981a408d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df\n",
    "n_clusters = [*range(2,11)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2531b4ad",
   "metadata": {},
   "source": [
    "### Kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888f4e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "param_km = [{'n_clusters':n_clusters}]\n",
    "pg = list(ParameterGrid(param_km))\n",
    "report_km = pd.DataFrame([],columns=['n clusters','inertia','silhouette_score'])\n",
    "\n",
    "for param in pg:\n",
    "    km = KMeans(n_clusters=param['n_clusters'],random_state=randomState)\n",
    "    y_km = km.fit_predict(X)\n",
    "    report_km.loc[len(report_km)] = [\n",
    "        param['n_clusters'],\n",
    "        km.inertia_,\n",
    "        silhouette_score(X,y_km)\n",
    "    ]\n",
    "\n",
    "display(report_km)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33675d3f",
   "metadata": {},
   "source": [
    "#### Checking for Hellbows - best solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f47a9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fix,ax = plt.subplots()\n",
    "ax.plot(n_clusters,report_km['inertia'],color='red')\n",
    "ax.set_xlabel('Number of clusters')\n",
    "ax.set_ylabel('inertia',color='red')\n",
    "\n",
    "ax2 = ax.twinx()\n",
    "ax2.plot(n_clusters,report_km['silhouette_score'],color='blue')\n",
    "ax2.set_ylabel('silhouette_score',color='blue')\n",
    "ax2.set_ylim(0,1)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Hellbows are the optimal number of clusters.\n",
    "# hellbows are where the inertia has a huge decrease and where the silhuette has a maximun"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd04792",
   "metadata": {},
   "source": [
    "### Aglomerative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f205108d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "n_clusters = [*range(2,7)]\n",
    "param_ac = [{\n",
    "    'n_clusters': n_clusters,\n",
    "    'linkage': ['ward','complete','avarage','single']\n",
    "}]\n",
    "pg = list(ParameterGrid(param_ac))\n",
    "result_ac = pd.DataFrame([],columns=['n_clusters','linkage','silhouette_score'])\n",
    "\n",
    "for param in pg:\n",
    "    ac = AgglomerativeClustering(\n",
    "        n_clusters=param['n_clusters'],\n",
    "        linkage=param['linkage']\n",
    "    )\n",
    "    y_ac = ac.fit_predict(X)\n",
    "    result_ac.loc[len(result_ac)]=[\n",
    "        param['n_clusters'],\n",
    "        param['linkage'],\n",
    "        silhouette_score(X,y_ac)\n",
    "    ]\n",
    "\n",
    "display(result_ac.sort_values(by='silhouette_score', ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828ecc64",
   "metadata": {},
   "source": [
    "### DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6015419a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "param_grid = {'eps': list(np.arange(0.001, 1, 0.005)), 'min_samples': list(range(2,10,1))}\n",
    "pg = list(ParameterGrid(param_grid))\n",
    "\n",
    "result_dbs = pd.DataFrame([],columns=['n_clusters','eps','min_samples','silhouette_score','unclust%'])\n",
    "\n",
    "for param in pg:\n",
    "    dbs = DBSCAN(eps=param['eps'],min_samples=param['min_samples'])\n",
    "    y_dbs = dbs.fit_predict(X)\n",
    "    cluster_labels_all = np.unique(y_dbs)\n",
    "    cluster_labels = cluster_labels_all[cluster_labels_all != -1] # -1 is the noise\n",
    "    n_clusters = len(cluster_labels)\n",
    "    if n_clusters > 1 and n_clusters < len(X):\n",
    "        # remove noise form the silhouette calcolus\n",
    "        X_cl = X.loc[y_dbs!=-1,:]\n",
    "        y_dbs_cl = y_dbs[y_dbs!=-1]\n",
    "        sil_score = silhouette_score(X_cl, y_dbs_cl)\n",
    "        uncl_p = (1 - y_dbs_cl.shape[0]/y_dbs.shape[0]) * 100 # percentage of unclustered points\n",
    "        result_dbs.loc[len(result_dbs)]=[\n",
    "            n_clusters,\n",
    "            param['eps'],\n",
    "            param['min_samples'],\n",
    "            sil_score,\n",
    "            uncl_p\n",
    "        ]\n",
    "\n",
    "\n",
    "##################### OR ############################\n",
    "param_dbs = [{\n",
    "    'eps': [*range(0.001,1,0.05)],\n",
    "    'min_samples': [*range(2,10)]\n",
    "}]\n",
    "pg_dbs = ParameterGrid(param_dbs)\n",
    "report_dbs = pd.DataFrame([],columns=['n_clusters','eps','min_samples','silouette_score','unclust%'])\n",
    "\n",
    "for param in pg_dbs:\n",
    "    dbs = DBSCAN(eps=param['eps'],min_samples=['min_samples'])\n",
    "    y_dbs = dbs.fit_predict(X)\n",
    "    y_dbs_clustered = y_dbs[y_dbs != -1, :]\n",
    "    X_dbs_clustered = X.loc[y_dbs != -1, :]\n",
    "    n_cluster = len(np.unique(y_dbs_clustered))\n",
    "    unclust = 1 - y_dbs_clustered.shape[0]/y_dbs.shape[0]\n",
    "    if n_cluster > 1 and n_cluster < len(X):\n",
    "        report_dbs.loc[len(report_dbs)]=[\n",
    "            n_cluster,\n",
    "            param['eps'],\n",
    "            param['min_samples'],\n",
    "            silhouette_score(X_dbs_clustered, y_dbs_clustered),\n",
    "            unclust*100\n",
    "        ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0bb6569",
   "metadata": {},
   "source": [
    "## Display Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe488ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pie chart of the cluster sizes\n",
    "clust_sizes_km = np.unique(y_km,return_counts=True)\n",
    "pd.DataFrame(clust_sizes_km[1]).plot.pie(y=0, autopct='%1.1f%%')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9991b650",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare clusters and features\n",
    "X['cluster_km']=y_km\n",
    "sns.pairplot(data=X, hue='cluster_km');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a677a838",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize DBSCAN results (the professor parameter generate 200 estimators, these is to show only the bests)\n",
    "\n",
    "sil_thr = 0.0  # visualize results only for combinations with silhouette above the threshold\n",
    "unc_thr = 22 # visualize results only for combinations with unclustered% below the threshold\n",
    "n_clu_max_thr = 5\n",
    "result_dbs[(result_dbs['silhouette']>=sil_thr)\\\n",
    "         & (result_dbs['unclust%']<=unc_thr)\\\n",
    "         & (result_dbs['n_clusters']<=n_clu_max_thr)].sort_values(['silhouette','unclust%'],ascending=[False,True])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLDM2526",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
